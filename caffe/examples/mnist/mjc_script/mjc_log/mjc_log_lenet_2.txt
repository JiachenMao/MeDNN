I0930 16:33:19.730608 15229 caffe.cpp:186] Using GPUs 0
I0930 16:33:21.149677 15229 caffe.cpp:191] GPU 0: GeForce GTX 1080
I0930 16:33:21.989311 15229 solver.cpp:50] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "lenet_mjc"
solver_mode: GPU
device_id: 0
net: "./lenet_train_test_grouplasso.prototxt"
block_group_decay: 0.001
I0930 16:33:21.989495 15229 solver.cpp:94] Creating training net from net file: ./lenet_train_test_grouplasso.prototxt
I0930 16:33:21.989862 15229 net.cpp:316] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0930 16:33:21.989878 15229 net.cpp:316] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0930 16:33:21.989994 15229 net.cpp:52] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "./mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 25
      ydimen: 5
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  connectivity_mode: CONNECTED
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 25
      ydimen: 5
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  connectivity_mode: CONNECTED
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 10
      ydimen: 10
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 10
      ydimen: 10
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0930 16:33:21.990067 15229 layer_factory.hpp:77] Creating layer mnist
I0930 16:33:21.991164 15229 net.cpp:94] Creating Layer mnist
I0930 16:33:21.991180 15229 net.cpp:402] mnist -> data
I0930 16:33:21.991227 15229 net.cpp:402] mnist -> label
I0930 16:33:21.992919 15247 db_lmdb.cpp:38] Opened lmdb ./mnist_train_lmdb
I0930 16:33:22.029943 15229 data_layer.cpp:41] output data size: 64,1,28,28
I0930 16:33:22.031069 15229 net.cpp:144] Setting up mnist
I0930 16:33:22.031118 15229 net.cpp:151] Top shape: 64 1 28 28 (50176)
I0930 16:33:22.031148 15229 net.cpp:151] Top shape: 64 (64)
I0930 16:33:22.031155 15229 net.cpp:159] Memory required for data: 200960
I0930 16:33:22.031174 15229 layer_factory.hpp:77] Creating layer conv1
I0930 16:33:22.031214 15229 net.cpp:94] Creating Layer conv1
I0930 16:33:22.031226 15229 net.cpp:428] conv1 <- data
I0930 16:33:22.031247 15229 net.cpp:402] conv1 -> conv1
I0930 16:33:22.036358 15229 net.cpp:144] Setting up conv1
I0930 16:33:22.036389 15229 net.cpp:151] Top shape: 64 20 24 24 (737280)
I0930 16:33:22.036396 15229 net.cpp:159] Memory required for data: 3150080
I0930 16:33:22.036442 15229 layer_factory.hpp:77] Creating layer pool1
I0930 16:33:22.036463 15229 net.cpp:94] Creating Layer pool1
I0930 16:33:22.036469 15229 net.cpp:428] pool1 <- conv1
I0930 16:33:22.036479 15229 net.cpp:402] pool1 -> pool1
I0930 16:33:22.038282 15229 net.cpp:144] Setting up pool1
I0930 16:33:22.038301 15229 net.cpp:151] Top shape: 64 20 12 12 (184320)
I0930 16:33:22.038307 15229 net.cpp:159] Memory required for data: 3887360
I0930 16:33:22.038314 15229 layer_factory.hpp:77] Creating layer conv2
I0930 16:33:22.038332 15229 net.cpp:94] Creating Layer conv2
I0930 16:33:22.038338 15229 net.cpp:428] conv2 <- pool1
I0930 16:33:22.038349 15229 net.cpp:402] conv2 -> conv2
I0930 16:33:22.042407 15229 net.cpp:144] Setting up conv2
I0930 16:33:22.042425 15229 net.cpp:151] Top shape: 64 50 8 8 (204800)
I0930 16:33:22.042431 15229 net.cpp:159] Memory required for data: 4706560
I0930 16:33:22.042451 15229 layer_factory.hpp:77] Creating layer pool2
I0930 16:33:22.042464 15229 net.cpp:94] Creating Layer pool2
I0930 16:33:22.042470 15229 net.cpp:428] pool2 <- conv2
I0930 16:33:22.042479 15229 net.cpp:402] pool2 -> pool2
I0930 16:33:22.044076 15229 net.cpp:144] Setting up pool2
I0930 16:33:22.044095 15229 net.cpp:151] Top shape: 64 50 4 4 (51200)
I0930 16:33:22.044100 15229 net.cpp:159] Memory required for data: 4911360
I0930 16:33:22.044106 15229 layer_factory.hpp:77] Creating layer ip1
I0930 16:33:22.044118 15229 net.cpp:94] Creating Layer ip1
I0930 16:33:22.044124 15229 net.cpp:428] ip1 <- pool2
I0930 16:33:22.044134 15229 net.cpp:402] ip1 -> ip1
I0930 16:33:22.050127 15229 net.cpp:144] Setting up ip1
I0930 16:33:22.050144 15229 net.cpp:151] Top shape: 64 500 (32000)
I0930 16:33:22.050149 15229 net.cpp:159] Memory required for data: 5039360
I0930 16:33:22.050169 15229 layer_factory.hpp:77] Creating layer relu1
I0930 16:33:22.050180 15229 net.cpp:94] Creating Layer relu1
I0930 16:33:22.050186 15229 net.cpp:428] relu1 <- ip1
I0930 16:33:22.050195 15229 net.cpp:389] relu1 -> ip1 (in-place)
I0930 16:33:22.050207 15229 net.cpp:144] Setting up relu1
I0930 16:33:22.050214 15229 net.cpp:151] Top shape: 64 500 (32000)
I0930 16:33:22.050218 15229 net.cpp:159] Memory required for data: 5167360
I0930 16:33:22.050223 15229 layer_factory.hpp:77] Creating layer ip2
I0930 16:33:22.050232 15229 net.cpp:94] Creating Layer ip2
I0930 16:33:22.050237 15229 net.cpp:428] ip2 <- ip1
I0930 16:33:22.050246 15229 net.cpp:402] ip2 -> ip2
I0930 16:33:22.050456 15229 net.cpp:144] Setting up ip2
I0930 16:33:22.050465 15229 net.cpp:151] Top shape: 64 10 (640)
I0930 16:33:22.050469 15229 net.cpp:159] Memory required for data: 5169920
I0930 16:33:22.050482 15229 layer_factory.hpp:77] Creating layer loss
I0930 16:33:22.050504 15229 net.cpp:94] Creating Layer loss
I0930 16:33:22.050510 15229 net.cpp:428] loss <- ip2
I0930 16:33:22.050518 15229 net.cpp:428] loss <- label
I0930 16:33:22.050528 15229 net.cpp:402] loss -> loss
I0930 16:33:22.050549 15229 layer_factory.hpp:77] Creating layer loss
I0930 16:33:22.050703 15229 net.cpp:144] Setting up loss
I0930 16:33:22.050711 15229 net.cpp:151] Top shape: (1)
I0930 16:33:22.050716 15229 net.cpp:154]     with loss weight 1
I0930 16:33:22.050750 15229 net.cpp:159] Memory required for data: 5169924
I0930 16:33:22.050756 15229 net.cpp:220] loss needs backward computation.
I0930 16:33:22.050766 15229 net.cpp:220] ip2 needs backward computation.
I0930 16:33:22.050771 15229 net.cpp:220] relu1 needs backward computation.
I0930 16:33:22.050799 15229 net.cpp:220] ip1 needs backward computation.
I0930 16:33:22.050806 15229 net.cpp:220] pool2 needs backward computation.
I0930 16:33:22.050811 15229 net.cpp:220] conv2 needs backward computation.
I0930 16:33:22.050815 15229 net.cpp:220] pool1 needs backward computation.
I0930 16:33:22.050820 15229 net.cpp:220] conv1 needs backward computation.
I0930 16:33:22.050825 15229 net.cpp:222] mnist does not need backward computation.
I0930 16:33:22.050830 15229 net.cpp:264] This network produces output loss
I0930 16:33:22.050844 15229 net.cpp:277] Network initialization done.
I0930 16:33:22.051105 15229 solver.cpp:184] Creating test net (#0) specified by net file: ./lenet_train_test_grouplasso.prototxt
I0930 16:33:22.051136 15229 net.cpp:316] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0930 16:33:22.051268 15229 net.cpp:52] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "./mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 25
      ydimen: 5
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  connectivity_mode: CONNECTED
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 25
      ydimen: 5
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  connectivity_mode: CONNECTED
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 10
      ydimen: 10
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 1
    breadth_decay_mult: 1
    kernel_shape_decay_mult: 1
    block_group_lasso {
      xdimen: 10
      ydimen: 10
      block_decay_mult: 1
    }
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0930 16:33:22.051350 15229 layer_factory.hpp:77] Creating layer mnist
I0930 16:33:22.051502 15229 net.cpp:94] Creating Layer mnist
I0930 16:33:22.051515 15229 net.cpp:402] mnist -> data
I0930 16:33:22.051527 15229 net.cpp:402] mnist -> label
I0930 16:33:22.053112 15249 db_lmdb.cpp:38] Opened lmdb ./mnist_test_lmdb
I0930 16:33:22.053663 15229 data_layer.cpp:41] output data size: 100,1,28,28
I0930 16:33:22.054651 15229 net.cpp:144] Setting up mnist
I0930 16:33:22.054668 15229 net.cpp:151] Top shape: 100 1 28 28 (78400)
I0930 16:33:22.054677 15229 net.cpp:151] Top shape: 100 (100)
I0930 16:33:22.054682 15229 net.cpp:159] Memory required for data: 314000
I0930 16:33:22.054687 15229 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0930 16:33:22.054699 15229 net.cpp:94] Creating Layer label_mnist_1_split
I0930 16:33:22.054705 15229 net.cpp:428] label_mnist_1_split <- label
I0930 16:33:22.054714 15229 net.cpp:402] label_mnist_1_split -> label_mnist_1_split_0
I0930 16:33:22.054728 15229 net.cpp:402] label_mnist_1_split -> label_mnist_1_split_1
I0930 16:33:22.055079 15229 net.cpp:144] Setting up label_mnist_1_split
I0930 16:33:22.055117 15229 net.cpp:151] Top shape: 100 (100)
I0930 16:33:22.055129 15229 net.cpp:151] Top shape: 100 (100)
I0930 16:33:22.055136 15229 net.cpp:159] Memory required for data: 314800
I0930 16:33:22.055147 15229 layer_factory.hpp:77] Creating layer conv1
I0930 16:33:22.055176 15229 net.cpp:94] Creating Layer conv1
I0930 16:33:22.055186 15229 net.cpp:428] conv1 <- data
I0930 16:33:22.055202 15229 net.cpp:402] conv1 -> conv1
I0930 16:33:22.061849 15229 net.cpp:144] Setting up conv1
I0930 16:33:22.061883 15229 net.cpp:151] Top shape: 100 20 24 24 (1152000)
I0930 16:33:22.061892 15229 net.cpp:159] Memory required for data: 4922800
I0930 16:33:22.061933 15229 layer_factory.hpp:77] Creating layer pool1
I0930 16:33:22.061951 15229 net.cpp:94] Creating Layer pool1
I0930 16:33:22.061961 15229 net.cpp:428] pool1 <- conv1
I0930 16:33:22.061975 15229 net.cpp:402] pool1 -> pool1
I0930 16:33:22.066911 15229 net.cpp:144] Setting up pool1
I0930 16:33:22.066942 15229 net.cpp:151] Top shape: 100 20 12 12 (288000)
I0930 16:33:22.066951 15229 net.cpp:159] Memory required for data: 6074800
I0930 16:33:22.066961 15229 layer_factory.hpp:77] Creating layer conv2
I0930 16:33:22.066987 15229 net.cpp:94] Creating Layer conv2
I0930 16:33:22.066998 15229 net.cpp:428] conv2 <- pool1
I0930 16:33:22.067013 15229 net.cpp:402] conv2 -> conv2
I0930 16:33:22.070982 15229 net.cpp:144] Setting up conv2
I0930 16:33:22.071012 15229 net.cpp:151] Top shape: 100 50 8 8 (320000)
I0930 16:33:22.071020 15229 net.cpp:159] Memory required for data: 7354800
I0930 16:33:22.071050 15229 layer_factory.hpp:77] Creating layer pool2
I0930 16:33:22.071066 15229 net.cpp:94] Creating Layer pool2
I0930 16:33:22.071075 15229 net.cpp:428] pool2 <- conv2
I0930 16:33:22.071089 15229 net.cpp:402] pool2 -> pool2
I0930 16:33:22.071406 15229 net.cpp:144] Setting up pool2
I0930 16:33:22.071422 15229 net.cpp:151] Top shape: 100 50 4 4 (80000)
I0930 16:33:22.071430 15229 net.cpp:159] Memory required for data: 7674800
I0930 16:33:22.071439 15229 layer_factory.hpp:77] Creating layer ip1
I0930 16:33:22.071455 15229 net.cpp:94] Creating Layer ip1
I0930 16:33:22.071463 15229 net.cpp:428] ip1 <- pool2
I0930 16:33:22.071477 15229 net.cpp:402] ip1 -> ip1
I0930 16:33:22.080724 15229 net.cpp:144] Setting up ip1
I0930 16:33:22.080751 15229 net.cpp:151] Top shape: 100 500 (50000)
I0930 16:33:22.080759 15229 net.cpp:159] Memory required for data: 7874800
I0930 16:33:22.080792 15229 layer_factory.hpp:77] Creating layer relu1
I0930 16:33:22.080807 15229 net.cpp:94] Creating Layer relu1
I0930 16:33:22.080816 15229 net.cpp:428] relu1 <- ip1
I0930 16:33:22.080832 15229 net.cpp:389] relu1 -> ip1 (in-place)
I0930 16:33:22.080847 15229 net.cpp:144] Setting up relu1
I0930 16:33:22.080857 15229 net.cpp:151] Top shape: 100 500 (50000)
I0930 16:33:22.080863 15229 net.cpp:159] Memory required for data: 8074800
I0930 16:33:22.080870 15229 layer_factory.hpp:77] Creating layer ip2
I0930 16:33:22.080889 15229 net.cpp:94] Creating Layer ip2
I0930 16:33:22.080898 15229 net.cpp:428] ip2 <- ip1
I0930 16:33:22.080909 15229 net.cpp:402] ip2 -> ip2
I0930 16:33:22.081270 15229 net.cpp:144] Setting up ip2
I0930 16:33:22.081285 15229 net.cpp:151] Top shape: 100 10 (1000)
I0930 16:33:22.081305 15229 net.cpp:159] Memory required for data: 8078800
I0930 16:33:22.081337 15229 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0930 16:33:22.081351 15229 net.cpp:94] Creating Layer ip2_ip2_0_split
I0930 16:33:22.081358 15229 net.cpp:428] ip2_ip2_0_split <- ip2
I0930 16:33:22.081377 15229 net.cpp:402] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0930 16:33:22.081392 15229 net.cpp:402] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0930 16:33:22.081511 15229 net.cpp:144] Setting up ip2_ip2_0_split
I0930 16:33:22.081523 15229 net.cpp:151] Top shape: 100 10 (1000)
I0930 16:33:22.081532 15229 net.cpp:151] Top shape: 100 10 (1000)
I0930 16:33:22.081538 15229 net.cpp:159] Memory required for data: 8086800
I0930 16:33:22.081545 15229 layer_factory.hpp:77] Creating layer accuracy
I0930 16:33:22.081562 15229 net.cpp:94] Creating Layer accuracy
I0930 16:33:22.081569 15229 net.cpp:428] accuracy <- ip2_ip2_0_split_0
I0930 16:33:22.081578 15229 net.cpp:428] accuracy <- label_mnist_1_split_0
I0930 16:33:22.081589 15229 net.cpp:402] accuracy -> accuracy
I0930 16:33:22.081632 15229 net.cpp:144] Setting up accuracy
I0930 16:33:22.081643 15229 net.cpp:151] Top shape: (1)
I0930 16:33:22.081650 15229 net.cpp:159] Memory required for data: 8086804
I0930 16:33:22.081656 15229 layer_factory.hpp:77] Creating layer loss
I0930 16:33:22.081671 15229 net.cpp:94] Creating Layer loss
I0930 16:33:22.081681 15229 net.cpp:428] loss <- ip2_ip2_0_split_1
I0930 16:33:22.081689 15229 net.cpp:428] loss <- label_mnist_1_split_1
I0930 16:33:22.081698 15229 net.cpp:402] loss -> loss
I0930 16:33:22.081713 15229 layer_factory.hpp:77] Creating layer loss
I0930 16:33:22.081971 15229 net.cpp:144] Setting up loss
I0930 16:33:22.081985 15229 net.cpp:151] Top shape: (1)
I0930 16:33:22.081991 15229 net.cpp:154]     with loss weight 1
I0930 16:33:22.082011 15229 net.cpp:159] Memory required for data: 8086808
I0930 16:33:22.082020 15229 net.cpp:220] loss needs backward computation.
I0930 16:33:22.082029 15229 net.cpp:222] accuracy does not need backward computation.
I0930 16:33:22.082038 15229 net.cpp:220] ip2_ip2_0_split needs backward computation.
I0930 16:33:22.082046 15229 net.cpp:220] ip2 needs backward computation.
I0930 16:33:22.082053 15229 net.cpp:220] relu1 needs backward computation.
I0930 16:33:22.082061 15229 net.cpp:220] ip1 needs backward computation.
I0930 16:33:22.082067 15229 net.cpp:220] pool2 needs backward computation.
I0930 16:33:22.082075 15229 net.cpp:220] conv2 needs backward computation.
I0930 16:33:22.082082 15229 net.cpp:220] pool1 needs backward computation.
I0930 16:33:22.082089 15229 net.cpp:220] conv1 needs backward computation.
I0930 16:33:22.082098 15229 net.cpp:222] label_mnist_1_split does not need backward computation.
I0930 16:33:22.082109 15229 net.cpp:222] mnist does not need backward computation.
I0930 16:33:22.082118 15229 net.cpp:264] This network produces output accuracy
I0930 16:33:22.082125 15229 net.cpp:264] This network produces output loss
I0930 16:33:22.082146 15229 net.cpp:277] Network initialization done.
I0930 16:33:22.082243 15229 solver.cpp:62] Solver scaffolding done.
I0930 16:33:22.090481 15229 caffe.cpp:220] Starting Optimization
I0930 16:33:22.090526 15229 solver.cpp:290] Solving LeNet
I0930 16:33:22.090534 15229 solver.cpp:291] Learning Rate Policy: inv
I0930 16:33:22.091753 15229 solver.cpp:348] Iteration 0, Testing net (#0)
I0930 16:33:22.784515 15229 solver.cpp:415]     Test net output #0: accuracy = 0.0923
I0930 16:33:22.784546 15229 solver.cpp:415]     Test net output #1: loss = 2.31947 (* 1 = 2.31947 loss)
I0930 16:33:22.798861 15229 solver.cpp:231] Iteration 0, loss = 2.27167
I0930 16:33:22.798914 15229 solver.cpp:247]     Train net output #0: loss = 2.27167 (* 1 = 2.27167 loss)
I0930 16:33:22.798970 15229 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0930 16:33:22.800334 15229 sgd_solver.cpp:120]     Element Sparsity %: 
0	100	0.124	100	0.16025	100	0.1	100	
I0930 16:33:22.800386 15229 sgd_solver.cpp:130]      Column Sparsity %: 
0	100	0	100	0	100	0	100	
I0930 16:33:22.800416 15229 sgd_solver.cpp:139]         Row Sparsity %: 
0	100	0	100	0	100	0	100	
I0930 16:33:22.800560 15229 sgd_solver.cpp:153]       Block Sparsity %: 
(25,5):0;		(25,5):0;		(10,10):0;		(10,10):0;		
F0930 16:33:22.800660 15229 sgd_solver.cpp:555] mjc: group lasso along blocks
*** Check failure stack trace: ***
    @     0x7f55a4cba5cd  google::LogMessage::Fail()
    @     0x7f55a4cbc433  google::LogMessage::SendToLog()
    @     0x7f55a4cba15b  google::LogMessage::Flush()
    @     0x7f55a4cbce1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f55a545c2fd  caffe::SGDSolver<>::GroupLassoRegularize()
    @     0x7f55a5457d88  caffe::SGDSolver<>::ApplyUpdate()
    @     0x7f55a54263e4  caffe::Solver<>::Step()
    @     0x7f55a5426dd9  caffe::Solver<>::Solve()
    @           0x40af47  train()
    @           0x407400  main
    @     0x7f55a3f49830  __libc_start_main
    @           0x407ab9  _start
    @              (nil)  (unknown)
Aborted (core dumped)
